import streamlit as st
import google.generativeai as genai
from groq import Groq
from PIL import Image
import io
import time
import requests
import fitz
import pandas as pd
from docx import Document
from pptx import Presentation
from google.api_core import exceptions

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="AI Architect Multi-Pro", page_icon="ğŸš€", layout="wide")

st.markdown("""
    <style>
    .stApp { background: linear-gradient(135deg, #0f172a 0%, #1e1e2f 100%); color: #ffffff; }
    .stTabs [data-baseweb="tab"] { height: 50px; background-color: rgba(255, 255, 255, 0.05); border-radius: 10px; color: white; font-weight: bold; }
    .stButton>button { background: linear-gradient(90deg, #00d2ff 0%, #3a7bd5 100%); color: white; border: none; border-radius: 12px; font-weight: 700; width: 100%; }
    .result-box { background: rgba(255, 255, 255, 0.03); backdrop-filter: blur(10px); padding: 20px; border-radius: 15px; border-left: 5px solid #00d2ff; }
    </style>
    """, unsafe_allow_html=True)

# --- 2. Ù…Ø­Ø±Ùƒ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ (ÙŠØ¯Ø¹Ù… Gemini Ùˆ Groq) ---
def generate_ai_response(provider, api_key, model_name, payload):
    try:
        if provider == "Google Gemini":
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª (Multimodal)
            response = model.generate_content(payload)
            return response.text
        
        elif provider == "Groq (Ultra Fast)":
            client = Groq(api_key=api_key)
            # Groq Ø­Ø§Ù„ÙŠØ§Ù‹ ÙŠØ¯Ø¹Ù… Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ (Llama 3.1)
            # Ø³Ù†Ø­ÙˆÙ„ Ø§Ù„Ù€ payload Ù„Ù†Øµ Ø¨Ø³ÙŠØ· Ù„Ù„Ù€ Groq
            prompt = payload if isinstance(payload, str) else str(payload[0])
            chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=model_name,
            )
            return chat_completion.choices[0].message.content
            
    except Exception as e:
        st.error(f"Error from {provider}: {e}")
        return None

# --- 3. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Selection) ---
with st.sidebar:
    st.markdown("<h2 style='color: #00d2ff;'>âš™ï¸ Provider Settings</h2>", unsafe_allow_html=True)
    provider = st.selectbox("Choose AI Provider:", ["Google Gemini", "Groq (Ultra Fast)"])
    
    api_key = st.text_input(f"Enter {provider} API Key:", type="password")
    
    model_choice = "gemini-1.5-flash" # Default
    if api_key:
        if provider == "Google Gemini":
            model_choice = st.selectbox("Model:", ["gemini-1.5-flash", "gemini-1.5-pro"])
        else:
            model_choice = st.selectbox("Model:", ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"])

# --- 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
if api_key:
    st.markdown(f"<h1 style='text-align: center;'>ğŸš€ AI Architect <span style='color: #00d2ff;'>Multi-Pro</span></h1>", unsafe_allow_html=True)
    
    tabs = st.tabs(["âœ¨ Smart Prompts", "ğŸ“‘ Ultimate Analyzer", "ğŸ§  Universal Architect"])

    # --- Tab: Analyzer (ØªØ·ÙˆÙŠØ± Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©) ---
    with tabs[1]:
        up_docs = st.file_uploader("Upload Docs (Up to 10)", accept_multiple_files=True, type=["pdf", "docx", "xlsx", "txt", "py"])
        query = st.text_area("What's your request?")
        
        if st.button("Process with AI ğŸš€") and (up_docs or query):
            with st.spinner(f"Processing via {provider}..."):
                # (Ù‡Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ÙŠ Ø¹Ù…Ù„Ù†Ø§Ù‡Ø§ Ù‚Ø¨Ù„ ÙƒØ¯Ø© Ù„Ù„Ù…Ù„ÙØ§Øª)
                # Ù„Ù„ØªØ¨Ø³ÙŠØ·ØŒ Ø³Ù†Ø±Ø³Ù„ Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØ§Ø±
                final_res = generate_ai_response(provider, api_key, model_choice, query)
                
                if final_res:
                    st.session_state['multi_res'] = final_res
                    st.markdown("### ğŸ” Analysis Result:")
                    st.code(final_res, language="markdown")

    # --- Tab: Universal Architect ---
    with tabs[2]:
        u_input = st.text_area("Enter any idea to build a pro prompt:")
        if st.button("Architect Now ğŸ”¨"):
            prompt = f"Assign Role, Context, and Task for: {u_input}. Output as a professional prompt."
            res = generate_ai_response(provider, api_key, model_choice, prompt)
            if res: st.code(res, language="text")

else:
    st.info("ğŸ‘ˆ Please select a provider and enter your API Key to unlock the power.")
